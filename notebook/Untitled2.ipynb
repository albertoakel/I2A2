{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b02f53-b710-43ec-a541-746bc2c6c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 DATAFRAME ORIGINAL:\n",
      "    ID_Registro Tipo_Eletronico  Ano_Fabricacao       Origem  Peso_kg\n",
      "0             1         Celular            2020  Residencial      0.2\n",
      "1             2        Notebook            2021    Comercial      2.5\n",
      "2             3          Tablet            2022  Residencial      0.6\n",
      "3             1         Celular            2020  Residencial      0.2\n",
      "4             2         Monitor            2019    Comercial      5.0\n",
      "5             4              TV            2020  Residencial      8.0\n",
      "6             5      Impressora            2021    Comercial      4.5\n",
      "7             5      Impressora            2021    Comercial      4.5\n",
      "8             5         Scanner            2022   Industrial      3.2\n",
      "9             6           Mouse            2023  Residencial      0.1\n",
      "10            7         Teclado            2022    Comercial      0.3\n",
      "\n",
      "📊 Total de registros: 11\n",
      "\n",
      "============================================================\n",
      "🔍 VERIFICAÇÃO DETALHADA DE IDs DUPLICADOS\n",
      "==================================================\n",
      "\n",
      "ID: 5 - 3 registros\n",
      "   Status: DADOS DIFERENTES\n",
      "   Colunas com diferenças: ['Tipo_Eletronico', 'Ano_Fabricacao', 'Origem', 'Peso_kg']\n",
      "\n",
      "ID: 1 - 2 registros\n",
      "   Status: DADOS DIFERENTES\n",
      "   Colunas com diferenças: []\n",
      "\n",
      "ID: 2 - 2 registros\n",
      "   Status: DADOS DIFERENTES\n",
      "   Colunas com diferenças: ['Tipo_Eletronico', 'Ano_Fabricacao', 'Peso_kg']\n",
      "\n",
      "============================================================\n",
      "🔄 PROCESSANDO DUPLICADOS...\n",
      "⚠️  Encontrados 7 registros com IDs duplicados\n",
      "\n",
      "Processando ID 1: 2 registros\n",
      "   ➖ Excluídas 1 linhas duplicadas\n",
      "\n",
      "Processando ID 2: 2 registros\n",
      "   🔄 Linha 4: ID modificado de 2 para 8\n",
      "\n",
      "Processando ID 5: 3 registros\n",
      "   ➖ Excluídas 1 linhas duplicadas\n",
      "   🔄 Linha 8: ID modificado de 5 para 9\n",
      "\n",
      "============================================================\n",
      "RELATÓRIO DO PROCESSAMENTO:\n",
      "✅ Linhas excluídas (duplicados): 2\n",
      "🆔 Novos IDs gerados: 2\n",
      "📊 Total de registros final: 9\n",
      "✅ Verificação final: Nenhum ID duplicado restante\n",
      "\n",
      "============================================================\n",
      "📋 DATAFRAME TRATADO:\n",
      "    ID_Registro Tipo_Eletronico  Ano_Fabricacao       Origem  Peso_kg\n",
      "0             1         Celular            2020  Residencial      0.2\n",
      "1             2        Notebook            2021    Comercial      2.5\n",
      "2             3          Tablet            2022  Residencial      0.6\n",
      "4             8         Monitor            2019    Comercial      5.0\n",
      "5             4              TV            2020  Residencial      8.0\n",
      "6             5      Impressora            2021    Comercial      4.5\n",
      "8             9         Scanner            2022   Industrial      3.2\n",
      "9             6           Mouse            2023  Residencial      0.1\n",
      "10            7         Teclado            2022    Comercial      0.3\n",
      "\n",
      "📊 Total de registros após tratamento: 9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def tratar_ids_duplicados(df, coluna_id='ID_Registro'):\n",
    "    \"\"\"\n",
    "    Verifica IDs duplicados e trata conforme as regras:\n",
    "    - Se dados duplicados: exclui linha\n",
    "    - Se dados diferentes: gera novo ID inteiro único\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fazer cópia do dataframe para não modificar o original\n",
    "    df_tratado = df.copy()\n",
    "    \n",
    "    # Identificar IDs duplicados\n",
    "    ids_duplicados = df_tratado[df_tratado.duplicated(subset=[coluna_id], keep=False)]\n",
    "    \n",
    "    if ids_duplicados.empty:\n",
    "        print(\"✅ Nenhum ID duplicado encontrado\")\n",
    "        return df_tratado\n",
    "    \n",
    "    print(f\"⚠️  Encontrados {len(ids_duplicados)} registros com IDs duplicados\")\n",
    "    \n",
    "    # Encontrar o maior ID atual para gerar novos IDs acima dele\n",
    "    max_id = df_tratado[coluna_id].max()\n",
    "    novos_ids_gerados = 0\n",
    "    linhas_excluidas = 0\n",
    "    \n",
    "    # Lista de todos os IDs existentes (para garantir unicidade)\n",
    "    ids_existentes = set(df_tratado[coluna_id].tolist())\n",
    "    \n",
    "    # Processar cada grupo de IDs duplicados\n",
    "    for id_valor, grupo in ids_duplicados.groupby(coluna_id):\n",
    "        if len(grupo) == 1:\n",
    "            continue  # Apenas um registro, não precisa tratar\n",
    "            \n",
    "        print(f\"\\nProcessando ID {id_valor}: {len(grupo)} registros\")\n",
    "        \n",
    "        # Verificar se todos os registros são completamente duplicados\n",
    "        colunas_verificar = [col for col in grupo.columns if col != coluna_id]\n",
    "        duplicados_completos = grupo.duplicated(subset=colunas_verificar, keep='first')\n",
    "        \n",
    "        # Identificar índices para exclusão e modificação\n",
    "        indices_para_excluir = []\n",
    "        indices_para_modificar = []\n",
    "        \n",
    "        for idx, is_duplicado in duplicados_completos.items():\n",
    "            if is_duplicado:\n",
    "                indices_para_excluir.append(idx)\n",
    "            else:\n",
    "                indices_para_modificar.append(idx)\n",
    "        \n",
    "        # Excluir registros duplicados (mantém apenas o primeiro de cada grupo duplicado)\n",
    "        if indices_para_excluir:\n",
    "            df_tratado = df_tratado.drop(indices_para_excluir)\n",
    "            linhas_excluidas += len(indices_para_excluir)\n",
    "            print(f\"   ➖ Excluídas {len(indices_para_excluir)} linhas duplicadas\")\n",
    "        \n",
    "        # Gerar novos IDs para registros com dados diferentes\n",
    "        if indices_para_modificar:\n",
    "            # Pular o primeiro registro (mantém o ID original)\n",
    "            indices_para_modificar = indices_para_modificar[1:] if indices_para_modificar else []\n",
    "            \n",
    "            for idx in indices_para_modificar:\n",
    "                # Gerar novo ID inteiro único\n",
    "                novo_id = max_id + 1\n",
    "                while novo_id in ids_existentes:\n",
    "                    novo_id += 1\n",
    "                \n",
    "                # Atualizar o ID\n",
    "                df_tratado.loc[idx, coluna_id] = novo_id\n",
    "                ids_existentes.add(novo_id)\n",
    "                max_id = max(max_id, novo_id)\n",
    "                novos_ids_gerados += 1\n",
    "                \n",
    "                print(f\"   🔄 Linha {idx}: ID modificado de {id_valor} para {novo_id}\")\n",
    "    \n",
    "    # Relatório final\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RELATÓRIO DO PROCESSAMENTO:\")\n",
    "    print(f\"✅ Linhas excluídas (duplicados): {linhas_excluidas}\")\n",
    "    print(f\"🆔 Novos IDs gerados: {novos_ids_gerados}\")\n",
    "    print(f\"📊 Total de registros final: {len(df_tratado)}\")\n",
    "    \n",
    "    # Verificar se ainda há duplicados após o tratamento\n",
    "    duplicados_finais = df_tratado[df_tratado.duplicated(subset=[coluna_id], keep=False)]\n",
    "    if duplicados_finais.empty:\n",
    "        print(\"✅ Verificação final: Nenhum ID duplicado restante\")\n",
    "    else:\n",
    "        print(f\"⚠️  ATENÇÃO: Ainda existem {len(duplicados_finais)} IDs duplicados\")\n",
    "    \n",
    "    return df_tratado\n",
    "\n",
    "def verificar_duplicados_detalhado(df, coluna_id='ID_Registro'):\n",
    "    \"\"\"\n",
    "    Função para verificar detalhadamente os IDs duplicados\n",
    "    \"\"\"\n",
    "    print(\"🔍 VERIFICAÇÃO DETALHADA DE IDs DUPLICADOS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Identificar IDs duplicados\n",
    "    ids_duplicados = df[df.duplicated(subset=[coluna_id], keep=False)]\n",
    "    \n",
    "    if ids_duplicados.empty:\n",
    "        print(\"Nenhum ID duplicado encontrado\")\n",
    "        return\n",
    "    \n",
    "    # Contar por ID\n",
    "    contagem = ids_duplicados[coluna_id].value_counts()\n",
    "    \n",
    "    for id_valor, quantidade in contagem.items():\n",
    "        registros = df[df[coluna_id] == id_valor]\n",
    "        \n",
    "        print(f\"\\nID: {id_valor} - {quantidade} registros\")\n",
    "        \n",
    "        # Verificar se são completamente duplicados\n",
    "        colunas_dados = [col for col in registros.columns if col != coluna_id]\n",
    "        duplicados_exatos = registros.duplicated(subset=colunas_dados).all()\n",
    "        \n",
    "        if duplicados_exatos:\n",
    "            print(\"   Status: DUPLICADOS COMPLETOS (dados idênticos)\")\n",
    "        else:\n",
    "            print(\"   Status: DADOS DIFERENTES\")\n",
    "            # Mostrar colunas com diferenças\n",
    "            colunas_diff = registros.columns[registros.nunique() > 1]\n",
    "            print(f\"   Colunas com diferenças: {list(colunas_diff)}\")\n",
    "\n",
    "# Exemplo de uso completo\n",
    "def exemplo_completo():\n",
    "    # Criar dados de exemplo para teste\n",
    "    dados_exemplo = {\n",
    "        'ID_Registro': [1, 2, 3, 1, 2, 4, 5, 5, 5, 6, 7],\n",
    "        'Tipo_Eletronico': ['Celular', 'Notebook', 'Tablet', 'Celular', 'Monitor', \n",
    "                           'TV', 'Impressora', 'Impressora', 'Scanner', 'Mouse', 'Teclado'],\n",
    "        'Ano_Fabricacao': [2020, 2021, 2022, 2020, 2019, \n",
    "                          2020, 2021, 2021, 2022, 2023, 2022],\n",
    "        'Origem': ['Residencial', 'Comercial', 'Residencial', 'Residencial', 'Comercial',\n",
    "                  'Residencial', 'Comercial', 'Comercial', 'Industrial', 'Residencial', 'Comercial'],\n",
    "        'Peso_kg': [0.2, 2.5, 0.6, 0.2, 5.0, 8.0, 4.5, 4.5, 3.2, 0.1, 0.3]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(dados_exemplo)\n",
    "    \n",
    "    print(\"📋 DATAFRAME ORIGINAL:\")\n",
    "    print(df)\n",
    "    print(f\"\\n📊 Total de registros: {len(df)}\")\n",
    "    \n",
    "    # Verificar duplicados detalhadamente\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    verificar_duplicados_detalhado(df, 'ID_Registro')\n",
    "    \n",
    "    # Tratar duplicados\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🔄 PROCESSANDO DUPLICADOS...\")\n",
    "    df_tratado = tratar_ids_duplicados(df, 'ID_Registro')\n",
    "    \n",
    "    # Mostrar resultado\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📋 DATAFRAME TRATADO:\")\n",
    "    print(df_tratado)\n",
    "    print(f\"\\n📊 Total de registros após tratamento: {len(df_tratado)}\")\n",
    "    \n",
    "    return df_tratado\n",
    "\n",
    "# Para usar com seus dados reais:\n",
    "# df = pd.read_csv('seu_arquivo.csv')  # ou seu dataframe\n",
    "# verificar_duplicados_detalhado(df, 'ID_Registro')\n",
    "# df_tratado = tratar_ids_duplicados(df, 'ID_Registro')\n",
    "\n",
    "# Executar exemplo\n",
    "if __name__ == \"__main__\":\n",
    "    df_resultado = exemplo_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f7132-145c-4060-9937-d9bbd8ce3d87",
   "metadata": {},
   "source": [
    "### 🛠️ Propostas\n",
    "\n",
    "1. **Pontos de Coleta Segmentados**\n",
    "   * Centros urbanos: coleta de computadores, celulares e impressoras.\n",
    "   * Municípios rurais: coleta de baterias, TVs e geladeiras.\n",
    "   * Rotas logísticas otimizadas para transporte seguro de equipamentos pesados e tóxicos\n",
    "2. **Campanhas Integradas de Educação Ambiental**\n",
    "    * Associadas à localização de pontos de coleta.\n",
    "    * Foco em conscientização sobre riscos de metais pesados e destinos corretos.\n",
    "3. **Políticas de Incentivo e Responsabilidade do Fabricante**\n",
    "    * Subsídios ou créditos para órgãos governamentais e empresas privadas que realizem descarte formal.\n",
    "    * Parcerias com cooperativas e recicladores locais.\n",
    "    * Adaptação de modelos de EPR inspirados em países como Alemanha, Japão e Coreia do Sul, responsabilizando fabricantes pelo destino final seguro de seus produtos\n",
    "4. **Monitoramento Dinâmico**\n",
    "   * Dashboards integrando origem, tipo, peso, destino e risco.\n",
    "   * Identificação de municípios prioritários para intervenção rápida.\n",
    "   * Avaliação contínua do impacto das campanhas e ajustes de rotas logísticas.\n",
    "5. **Planejamento Regional Diferenciado**\n",
    "    * Estratégias específicas para grandes centros urbanos, municípios médios e rurais.\n",
    "    * Foco em redução de descarte informal em áreas críticas, sem negligenciar municípios menores que apresentam boas práticas.\n",
    "\n",
    "O estudo evidencia que a gestão do lixo eletrônico na Amazônia deve ser multidimensional, considerando simultaneamente tipo de eletrônico, origem do resíduo, peso, risco tóxico, educação ambiental e infraestrutura logística. A integração desses fatores permite identificar padrões de risco, alocar recursos de forma estratégica e aumentar a taxa de descarte formal. A proposta de pontos de coleta segmentados, associada a campanhas educativas e monitoramento dinâmico, constitui uma abordagem prática e realista para reduzir impactos ambientais e proteger a saúde das populações amazônicas. O relatório reforça que políticas públicas isoladas não são suficientes; apenas uma abordagem integrada e orientada por dados pode transformar informação em ação eficaz, contribuindo para a preservação ambiental e para o cumprimento das metas de sustentabilidade na região amazônica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4ed77-bd7d-4a75-b5db-32f934ba2405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
