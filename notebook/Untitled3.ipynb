{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc720741-665c-4ea0-b5d1-8329a196e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Arquivo carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "color = sns.color_palette('RdBu_r')\n",
    "\n",
    "# Leitura dos dados\n",
    "# Inserção de tratamento de exceção para controle de erros\n",
    "try:\n",
    "    path = '/home/akel/PycharmProjects/I2A2/data/raw/'\n",
    "    file = 'desafio6_lixo_eletronico.csv'\n",
    "    dfo = pd.read_csv(path + file)\n",
    "    print(\"✅ Arquivo carregado com sucesso!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Erro: Arquivo não encontrado. Verifique o caminho e tente novamente.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"❌ Erro: O arquivo está vazio.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(\"❌ Erro: Problema ao analisar o arquivo CSV. Verifique o formato.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro inesperado ao carregar o arquivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48c350a8-91ce-4095-823d-5ee91eb29907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Encontrados 10 registros com IDs duplicados\n",
      "\n",
      "Processando ID 7: 2 registros\n",
      "   🔄 Linha 14: ID modificado de 7 para 150\n",
      "\n",
      "Processando ID 22: 2 registros\n",
      "   🔄 Linha 44: ID modificado de 22 para 151\n",
      "\n",
      "Processando ID 37: 2 registros\n",
      "   🔄 Linha 74: ID modificado de 37 para 152\n",
      "\n",
      "Processando ID 52: 2 registros\n",
      "   🔄 Linha 104: ID modificado de 52 para 153\n",
      "\n",
      "Processando ID 67: 2 registros\n",
      "   🔄 Linha 134: ID modificado de 67 para 154\n",
      "================================================================================\n",
      "🔍 REGISTROS DUPLICADOS - ID_Registro\n",
      "================================================================================\n",
      "✅ Nenhum registro duplicado encontrado na coluna ID_Registro\n"
     ]
    }
   ],
   "source": [
    "def tratar_ids_duplicados(df, coluna_id='ID_Registro'):\n",
    "    \"\"\"\n",
    "    Verifica IDs duplicados e trata conforme as regras:\n",
    "    - Se dados duplicados: exclui linha\n",
    "    - Se dados diferentes: gera novo ID inteiro único\n",
    "    \"\"\"\n",
    "    # Fazer cópia do dataframe para não modificar o original\n",
    "    df_tratado = df.copy()\n",
    "    \n",
    "    # Identificar IDs duplicados\n",
    "    ids_duplicados = df_tratado[df_tratado.duplicated(subset=[coluna_id], keep=False)]\n",
    "    \n",
    "    if ids_duplicados.empty:\n",
    "        print(\"✅ Nenhum ID duplicado encontrado\")\n",
    "        return df_tratado\n",
    "    \n",
    "    print(f\"⚠️  Encontrados {len(ids_duplicados)} registros com IDs duplicados\")\n",
    "    \n",
    "    # Encontrar o maior ID atual para gerar novos IDs acima dele\n",
    "    max_id = df_tratado[coluna_id].max()\n",
    "    novos_ids_gerados = 0\n",
    "    linhas_excluidas = 0\n",
    "    \n",
    "    # Lista de todos os IDs existentes (para garantir unicidade)\n",
    "    ids_existentes = set(df_tratado[coluna_id].tolist())\n",
    "    \n",
    "    # Processar cada grupo de IDs duplicados\n",
    "    for id_valor, grupo in ids_duplicados.groupby(coluna_id):\n",
    "        if len(grupo) == 1:\n",
    "            continue  # Apenas um registro, não precisa tratar\n",
    "            \n",
    "        print(f\"\\nProcessando ID {id_valor}: {len(grupo)} registros\")\n",
    "        \n",
    "        # Verificar se todos os registros são completamente duplicados\n",
    "        colunas_verificar = [col for col in grupo.columns if col != coluna_id]\n",
    "        duplicados_completos = grupo.duplicated(subset=colunas_verificar, keep='first')\n",
    "        \n",
    "        # Identificar índices para exclusão e modificação\n",
    "        indices_para_excluir = []\n",
    "        indices_para_modificar = []\n",
    "        \n",
    "        for idx, is_duplicado in duplicados_completos.items():\n",
    "            if is_duplicado:\n",
    "                indices_para_excluir.append(idx)\n",
    "            else:\n",
    "                indices_para_modificar.append(idx)\n",
    "        \n",
    "        # Excluir registros duplicados (mantém apenas o primeiro de cada grupo duplicado)\n",
    "        if indices_para_excluir:\n",
    "            df_tratado = df_tratado.drop(indices_para_excluir)\n",
    "            linhas_excluidas += len(indices_para_excluir)\n",
    "            print(f\"   ➖ Excluídas {len(indices_para_excluir)} linhas duplicadas\")\n",
    "        \n",
    "        # Gerar novos IDs para registros com dados diferentes\n",
    "        if indices_para_modificar:\n",
    "            # Pular o primeiro registro (mantém o ID original)\n",
    "            indices_para_modificar = indices_para_modificar[1:] if indices_para_modificar else []\n",
    "            \n",
    "            for idx in indices_para_modificar:\n",
    "                # Gerar novo ID inteiro único\n",
    "                novo_id = max_id + 1\n",
    "                while novo_id in ids_existentes:\n",
    "                    novo_id += 1\n",
    "                \n",
    "                # Atualizar o ID\n",
    "                df_tratado.loc[idx, coluna_id] = novo_id\n",
    "                ids_existentes.add(novo_id)\n",
    "                max_id = max(max_id, novo_id)\n",
    "                novos_ids_gerados += 1\n",
    "                \n",
    "                print(f\"   🔄 Linha {idx}: ID modificado de {id_valor} para {novo_id}\")\n",
    "    return df_tratado\n",
    "\n",
    "df = tratar_ids_duplicados(dfo, 'ID_Registro')\n",
    "\n",
    "\n",
    "# Identificar e exibir registros duplicados baseados no ID_Registro\n",
    "duplicados = df[df.duplicated(subset=['ID_Registro'], keep=False)]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"🔍 REGISTROS DUPLICADOS - ID_Registro\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(duplicados) > 0:\n",
    "    print(f\"Total de registros duplicados: {len(duplicados)}\")\n",
    "    print(f\"IDs únicos duplicados: {duplicados['ID_Registro'].nunique()}\")\n",
    "    print(\"\\n📋 Registros duplicados:\")\n",
    "    print(display(duplicados.sort_values('ID_Registro')))\n",
    "else:\n",
    "    print(\"✅ Nenhum registro duplicado encontrado na coluna ID_Registro\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
