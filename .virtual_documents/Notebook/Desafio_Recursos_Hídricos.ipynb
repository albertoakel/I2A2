


import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import itertools as itera
import seaborn as sns
import plotly.express as px
import warnings
from matplotlib.ticker import MultipleLocator
import time
import matplotlib.ticker as ticker
inicio = time.time()

plt.rcParams["figure.dpi"] = 180
sns.set_style("whitegrid")
pd.set_option('display.width',None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_colwidth',None)

warnings.filterwarnings("ignore", category=FutureWarning)





data_dir='/home/akel/PycharmProjects/I2A2/data/'
dfc = pd.read_csv(data_dir+'base_climatica.csv')
dfs = pd.read_csv(data_dir+'base_socioeconomica.csv')
display(dfc.head(5))
display(dfs.head(5))
print(dfc.collunns)





# 1 - conversão e ordenando valores por datas
dfc['data'] = pd.to_datetime(dfc['data'])
dfs['data'] = pd.to_datetime(dfs['data'])

dfc.sort_values(by='data', inplace=True)
dfs.sort_values(by='data', inplace=True)

# 2-Remoção de linhas duplicadas
dfc.drop_duplicates(inplace=True)
dfs.drop_duplicates(inplace=True)

# 3- Padronização categóricas sim/nao -> 1/0
dfc['variacao_climatica']=dfc['variacao_climatica'].str.lower().map({'sim':1, 'não':0, 'nao':0})
dfs['acesso_agua_potavel']=dfs['acesso_agua_potavel'].str.lower().map({'sim':1, 'não':0, 'nao':0})

# 5. Garantir apenas valores numéricos positivos(assumir que valores negativos são erros de digitação).
num_dfc_p=dfc.select_dtypes(include=np.number).abs()
num_dfs_p=dfs.select_dtypes(include=np.number).abs()

dfc[num_dfc_p.columns] = num_dfc_p
dfs[num_dfs_p.columns] = num_dfs_p

dfc.reset_index(drop=True, inplace=True)
dfs.reset_index(drop=True, inplace=True)


# #FOCAR NAS DATAS COMUNS (abordagem mais conservadora)
datas_comuns = pd.Index(dfc['data']).intersection(dfs['data'])
dfc_dif = pd.Index(dfc['data']).difference(dfs['data'])
dfs_dif = pd.Index(dfs['data']).difference(dfc['data'])

# merge dataset #  Gerado e adaptado via deepseek

df_mrg= pd.merge(
    dfc,
    dfs,
    on='data',                 # chave de junção
    how='inner',               # garante apenas datas presentes nos dois
    suffixes=('_clima', '_socio')  # evita nomes duplicados
).sort_values('data').reset_index(drop=True)






#função para remoção de outliers (função gerada em deepseek)
def clean_outliers(df, z_threshold=3, iqr_factor=1.5):
    """
    Identifica e remove outliers de um DataFrame  usando múltiplos métodos.
    
    Parâmetros:
    - df: DataFrame com dados climáticos
    - z_threshold: Limiar para o método Z-score (padrão: 3)
    - iqr_factor: Fator para o método IQR (padrão: 1.5)
    
    Retorna:
    - DataFrame limpo sem outliers
    - DataFrame com outliers removidos
    """

    # Cópia do DataFrame original
    df_clean = df.copy()
 
    # Identificação de outliers para cada coluna numérica
    numeric_cols =df.select_dtypes(include=np.floating).columns
    
    outliers_mask = pd.DataFrame(index=df.index, columns=numeric_cols)
    
    for col in numeric_cols:
        # Método Z-score
        z_scores = np.abs(stats.zscore(df_clean[col]))
        z_outliers = z_scores > z_threshold
        
        # Método IQR
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        iqr_outliers = (df_clean[col] < (Q1 - iqr_factor * IQR)) | (df_clean[col] > (Q3 + iqr_factor * IQR))
        
        # Combina os dois métodos
        outliers_mask[col] = z_outliers | iqr_outliers
    
    # 3. Marcar linhas com outliers em múltiplas colunas
    total_outliers = outliers_mask.sum(axis=1)
    rows_to_remove = total_outliers[total_outliers >= 2].index  # Remove se tiver outlier em ≥2 colunas
    
    # 4. Criar DataFrames resultantes
    df_outliers_removed = df_clean.drop(rows_to_remove)
    df_only_outliers = df_clean.loc[rows_to_remove]
    
    return df_outliers_removed, df_only_outliers, numeric_cols 


# Aplicando a função
df3, df_outliers, numeric_cols  = clean_outliers(df_mrg)
# Resultados
print("=== DataFrame Limpo (sem outliers) ===")
print(f"Linhas originais: {len(df_mrg)}")
print(f"Linhas após limpeza: {len(df3)}")
print(f"Outliers removidos: {len(df_outliers)}\n")


# Plotagem adaptada via deepseek de outro codigo proprietário simular.
df_mrg['origem'] = 'original'
df3['origem'] = 'filter'

# Concatenar os DataFrames
dfx = pd.concat([df_mrg, df3])

fig, axs = plt.subplots(2, 3, figsize=(12, 7))
sns.boxplot(data=dfx,x='origem',y='chuvas_reais_mm',ax=axs[0,0])
axs[0,0].set_title("chuvas_reais", pad=12)

sns.boxplot(data=dfx,x='origem',y='temperatura_media_C',ax=axs[0,1])
axs[0,1].set_title("temperatura media", pad=12)

sns.boxplot(data=dfx,x='origem',y='indice_umidade_solo',ax=axs[0,2])
axs[0,2].set_title("indice umidade solo", pad=12)
plt.tight_layout(rect=[0, 0, 1, 0.96])

sns.boxplot(data=dfx,x='origem',y='volume_producao_tons',ax=axs[1,0])
axs[1,0].set_title("volume_producao_tons", pad=12)
axs[1,0].set_yscale('log')

sns.boxplot(data=dfx,x='origem',y='incidencia_doencas',ax=axs[1,1])
axs[1,1].set_title("incidencia_doencas", pad=12)
axs[1,1].set_yscale('log')

sns.boxplot(data=dfx,x='origem',y='indicador_seguranca_alimentar',ax=axs[1,2])
axs[1,2].set_title("indicador_seguranca_alimentar", pad=12)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.tight_layout()
#plt.savefig('figura1_boxplots.png', dpi=300)

plt.show()

nulos = pd.DataFrame(df3.iloc[:,1:10].isnull().sum()).T
nulos.index = ['nulls']
tipos = pd.DataFrame(df3.iloc[:,1:10].dtypes).T
tipos.index = ['types']

descricao_full = pd.concat([tipos, nulos,df3.describe()])






# Histogramas Gerado via gemini 2.5flash
# Configurar o estilo dos gráficos
sns.set(style="whitegrid")

# Lista das colunas e títulos
colunas = [
    'chuvas_reais_mm', 'temperatura_media_C', 'indice_umidade_solo',
    'volume_producao_tons', 'incidencia_doencas', 'indicador_seguranca_alimentar'
]
titulos = [
    'Chuvas Reais (mm)', 'Temperatura Média (°C)', 'Índice Umidade Solo (%)',
    'Volume Produção (ton)', 'Incidência Doenças', 'Segurança Alimentar'
]

# Estatísticas vetorizadas
df_estatisticas = pd.DataFrame({
    'Média': df3[colunas].mean(),
    'Mediana': df3[colunas].median(),
    'Desvio Padrão': df3[colunas].std(),
    'Q1 (25%)': df3[colunas].quantile(0.25),
    'Q3 (75%)': df3[colunas].quantile(0.75),
    'IQR': df3[colunas].quantile(0.75) - df3[colunas].quantile(0.25),
    'Min': df3[colunas].min(),
    'Max': df3[colunas].max()
})

# Defina o número de bins para cada variável
bins_var = [20, 20, 15, 20, 6, 20]

# Histogramas (bins e counts) usando list comprehensions com bins variáveis
hist_bins_counts = [
    np.histogram(df3[c].dropna(), bins=b) for c, b in zip(colunas, bins_var)
]

# DataFrame dos histogramas
histogram_df = pd.concat([
    pd.DataFrame({
        'bin_start': bins[:-1],
        'bin_end': bins[1:],
        'count': counts,
        'variable': var
    })
    for (counts, bins), var in zip(hist_bins_counts, colunas)
], ignore_index=True)

# Plot dos histogramas em subplots sem for e sem função customizada
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

list(map(lambda i: [
    sns.histplot(df3[colunas[i]], color='dodgerblue', bins=bins_var[i], alpha=0.7, ax=axes[i]),
    axes[i].axvline(df3[colunas[i]].mean(), color='red', linestyle='--', linewidth=1.5, label=f'Média: {df3[colunas[i]].mean():.2f}'),
    axes[i].axvline(df3[colunas[i]].median(), color='green', linestyle='-', linewidth=1.5, label=f'Mediana: {df3[colunas[i]].median():.2f}'),
    axes[i].set_title(titulos[i], fontsize=12, pad=10),
    axes[i].set_xlabel(''),
    axes[i].set_ylabel('Frequência', fontsize=10),
    axes[i].legend(fontsize=9),
    axes[i].set_xlim(df3[colunas[i]].quantile(0.05), df3[colunas[i]].quantile(0.95))
], range(len(colunas))))

plt.tight_layout(pad=3.0)
plt.suptitle('Distribuição das Variáveis Climáticas e Agrícolas com bins variáveis', y=1.02, fontsize=14)
#plt.savefig('figura2_histogram.png', dpi=300)

plt.show()






# Distribuição variavel categorica
p1= df3['variacao_climatica'].value_counts(normalize=True) * 100
p2= df3['acesso_agua_potavel'].value_counts(normalize=True) * 100

fig, ax = plt.subplots(1, 2, figsize=(12, 4))
bars = ax[0].bar(p1.index, p1.values, color=['tomato','dodgerblue'], 
                  edgecolor='black', width=0.6)
ax[0].set_title('variacao_climatica')
for bar, percentage in zip(bars, p1.values):
    ax[0].text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{percentage:.1f}%', 
                ha='center', va='bottom', fontsize=10)
ax[0].set_xticks(p1.index)
ax[0].set_xticklabels(['NÃO','SIM'])
ax[0].set_ylim([0,60])

bars = ax[1].bar(p1.index, p2.values, color=['tomato','dodgerblue'], 
                  edgecolor='black', width=0.6)
ax[1].set_title('acesso_agua_potavel')
for bar, percentage in zip(bars, p2.values):
    ax[1].text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{percentage:.1f}%', 
                ha='center', va='bottom', fontsize=10)
ax[1].set_xticks(p2.index)
ax[1].set_xticklabels(['NÃO','SIM'])
ax[1].set_ylim([0,60])
plt.suptitle('Distribuição das Variáveis categoricas', y=1.02, fontsize=14)

plt.show()





import scipy.stats as stats
#a hipótese de normalidade pode ser  aceita (p-valor > 0.05) ou rejeitada (p-valor ≤ 0.05).
# Lista das colunas para teste de normalidade
colunas = ['chuvas_reais_mm', 'temperatura_media_C', 'indice_umidade_solo',
           'volume_producao_tons', 'incidencia_doencas', 'indicador_seguranca_alimentar']

# Dicionário para armazenar os resultados dos testes
normality_tests = {}

# Teste de normalidade usando Shapiro-Wilk
for col in colunas:
    data = df3[col].dropna()
    stat, p_value = stats.shapiro(data)
    normality_tests[col] = {
        'statistic': stat,
        'p_value': p_value,
        'normal': p_value > 0.05
    }

# Exibir resultados
print("\nRESULTADOS DO TESTE DE NORMALIDADE (Shapiro-Wilk):")
for var, res in normality_tests.items():
    print(f"{var}: stat={res['statistic']:.4f}, p={res['p_value']:.4f}, normal={res['normal']}")
df_normalidade = pd.DataFrame(normality_tests).T






# Criar figuras e subplots
fig, axs = plt.subplots(6, 1, figsize=(12, 14))  
fig.subplots_adjust(hspace=0)

sim_data = df3[df3['variacao_climatica'] == 1]
sim_data2 = df3[df3['acesso_agua_potavel'] == 1]


# Gráfico 1: Chuvas Previstas
axs[0].plot(df3['data'].values, df3['chuvas_previstas_mm'].values, 
         'k--', markersize=4, label='Chuvas Previstas (mm)')
axs[0].scatter(sim_data['data'], sim_data['chuvas_previstas_mm'].values+20, 
            color='red', label='Evento (Sim)', marker='v', s=60)
axs[0].plot(df3['data'].values, df3['chuvas_reais_mm'].values, 
         'red', marker='o', markersize=4)
axs[0].set_ylabel('chuvas [mm]')
axs[0].grid(True, alpha=0.3)
axs[0].legend(loc='lower right')

# Formatar o eixo x para mostrar ticks de 10 em 10 dias
axs[0].xaxis.set_major_locator(ticker.MultipleLocator(10))
axs[0].tick_params(axis='x', rotation=45)

# Gráfico 2: Temperatura Média
axs[1].plot(df3['data'].values, df3['temperatura_media_C'].values,
         'g-', marker='o', markersize=4, label='Temperatura Média (°C)')

axs[1].set_ylabel('Temperatura Média (°C)')
axs[1].grid(True, alpha=0.3)
axs[1].xaxis.set_major_locator(ticker.MultipleLocator(10))
axs[1].tick_params(axis='x', rotation=45)


# Gráfico 3: Índice de Umidade do Solo
axs[2].plot(df3['data'].values, df3['indice_umidade_solo'].values,
         'b-', marker='o', markersize=4, label='Índice Umidade Solo')
axs[2].set_ylabel('Índice Umidade Solo')
axs[2].set_xlabel('Data')
axs[2].grid(True, alpha=0.3)
axs[2].xaxis.set_major_locator(ticker.MultipleLocator(10))
axs[2].tick_params(axis='x', rotation=45)

#volume_producao_tons	incidencia_doencas	acesso_agua_potavel	indicador_seguranca_alimentar

# Gráfico 4: produção toneladas
axs[3].plot(df3['data'].values, df3['volume_producao_tons'].values, 
         'orange', marker='x',markersize=4, label='volume_producao_tons')
axs[3].set_ylabel('producao [t]')
axs[3].grid(True, alpha=0.3)
axs[3].xaxis.set_major_locator(ticker.MultipleLocator(10))
axs[3].tick_params(axis='x', rotation=45)

# Gráfico 5: produção toneladas
axs[4].plot(df3['data'].values, df3['incidencia_doencas'].values, 
         'purple', marker='x',markersize=4, label='incidencia_doencas')
axs[4].set_ylabel('incidencia doencas')
axs[4].grid(True, alpha=0.3)
axs[4].xaxis.set_major_locator(ticker.MultipleLocator(10))
axs[4].tick_params(axis='x', rotation=45)

# Gráfico 5: produção toneladas
axs[5].plot(df3['data'].values, df3['indicador_seguranca_alimentar'].values, 
         'brown', marker='x',markersize=4, label='indicador_seguranca_alimentar')
axs[5].scatter(sim_data2['data'], sim_data2['indicador_seguranca_alimentar'].values+20, 
            color='brown', label='Evento (Sim)', marker='v', s=60)
axs[5].set_ylabel('seguranca alimentar')
axs[5].grid(True, alpha=0.3)
axs[5].xaxis.set_major_locator(ticker.MultipleLocator(10))
axs[5].tick_params(axis='x', rotation=45)
axs[5].legend(loc='lower right')

#plt.savefig('figura4_series.png', dpi=300)

plt.show()





#Correlação de spearmanr+ p-values
from scipy.stats import pearsonr,spearmanr
# def corr_pvals(df):
#     """Retorna dois DataFrames: r e p-value."""
#     cols = df.columns
#     r   = pd.DataFrame(np.eye(len(cols)), index=cols, columns=cols)
#     p   = pd.DataFrame(np.zeros_like(r),  index=cols, columns=cols)

#     for i in range(len(cols)):
#         for j in range(i+1, len(cols)):
#             mask = df[[cols[i], cols[j]]].dropna()
#             r_ij, p_ij = pearsonr(mask[cols[i]], mask[cols[j]])
#             r.iloc[i, j] = r_ij
#             r.iloc[j, i] = r_ij
#             p.iloc[i, j] = p_ij
#             p.iloc[j, i] = p_ij
#     return r, p

def corr_spearman_pvals(df):
    """Retorna dois DataFrames: coeficiente de correlação de Spearman e p-value."""
    cols = df.columns
    rho = pd.DataFrame(np.eye(len(cols)), index=cols, columns=cols)  # ρ (rho) para Spearman
    p = pd.DataFrame(np.zeros_like(rho), index=cols, columns=cols)
    
    for i in range(len(cols)):
        for j in range(i+1, len(cols)):
            mask = df[[cols[i], cols[j]]].dropna()
            rho_ij, p_ij = spearmanr(mask[cols[i]], mask[cols[j]])
            rho.iloc[i, j] = rho_ij
            rho.iloc[j, i] = rho_ij
            p.iloc[i, j] = p_ij
            p.iloc[j, i] = p_ij
    return rho, p    





# interpolação ( para avaliar a correlação person de forma efetiva)
df3['data'] = pd.to_datetime(df3['data'], errors='coerce')
df3_interp = (
    df3
    .drop(columns='origem', errors='ignore')
    .sort_values('data')
    .set_index('data')
    .interpolate(method='time')) 

R, P = corr_spearman_pvals(df3_interp)
# Correlações significativas (α=5 %)
sig   = (P < 0.05) & (np.abs(R) > 0.20)
rel   = R.where(sig).round(2).dropna(how='all', axis=0).dropna(how='all', axis=1)

R[R >=0.99] = pd.NA
R[(R < 0.2)&(R > -0.2)] = pd.NA

plt.figure(figsize=(15, 8))
sns.heatmap(R, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('figura 6: Matriz de Correlação |R|>0.2 & alpha=0.05')
plt.tight_layout(rect=[0.0, 0, 1, 0.96])
plt.savefig('figura5_mapadecalor.png', dpi=300)


plt.show()
# print(R.to_markdown())
# display(P.to_markdown())
df3_interp





#funçao criada via deepseek

def xcorr(a, b, max_lag=30):
    """Retorna (lags, rs) entre duas séries com defasagem."""
    lags = np.arange(-max_lag, max_lag+1)
    rs = [a.corr(b.shift(l)) for l in lags]
    return lags, rs

# Parâmetros do teste
n = len(df3_interp)  # tamanho da amostra
alpha_5 = 0.20  # limiar para significância ao nível de 5%

# Geração dos pares
pairs = list(itera.combinations(df3_interp.columns, 2))

# Lista para guardar 2 resultados para cada par
resultados = []

for var_x, var_y in pairs:
    lags, rs = xcorr(df3_interp[var_x], df3_interp[var_y], max_lag=30)
    lags = np.array(lags)
    rs = np.array(rs)

    # Encontrar os 2 maiores |r| significativos
    idx_ordenado = np.argsort(-np.abs(rs))  # do maior |r| para menor
    selecionados = 0

    for idx in idx_ordenado:
        r = rs[idx]
        lag = lags[idx]
        if abs(r) >= alpha_5:
            resultados.append({
                'var_x': var_x,
                'var_y': var_y,
                'lag': lag,
                'r': r,
                'tipo': 'positivo' if r > 0 else 'negativo',
                'significativo': True
            })
            selecionados += 1
        if selecionados == 2:
            break

# Transformar em DataFrame
df_relatorio = pd.DataFrame(resultados)

# Ordenar 10 pares com maires r
#df_relatorio.sort_values(by='r',ascending=False).head(7)
#df_relatorio







#funçao adatada via deepseek

# ------------------------------------------------------------------
# 1. Escolher os 5 pares com |r| mais altos
top_pairs = (
    df_relatorio
    .loc[df_relatorio['significativo']]          # só pares significativos
    .assign(abs_r=lambda d: d['r'].abs())        # coluna auxiliar
    .sort_values('abs_r', ascending=False)       # ordenar por |r|
    .drop_duplicates(subset=['var_x', 'var_y'])  # evitar repetições
    .head(5)
)

# ------------------------------------------------------------------
# 2. Gerar a figura com 5 subplots
fig, axs = plt.subplots(5, 1, figsize=(12, 12))
fig.subplots_adjust(hspace=0.45)

for idx, (ax, row) in enumerate(zip(axs, top_pairs.itertuples(index=False))):
    var_x, var_y = row.var_x, row.var_y

    # Recalcular a série de correlação cruzada completa
    lags, rs = xcorr(df3_interp[var_x], df3_interp[var_y], max_lag=30)

    # Stem‑plot
    ax.stem(lags, rs, basefmt=" ", use_line_collection=True)
    ax.axhline( +0.20, ls='--', color='gray', alpha=0.7)
    ax.axhline( -0.20, ls='--', color='gray', alpha=0.7)

    # Destaques: melhor correlação e respectivo lag
    best_lag = row.lag
    best_r   = row.r
    ax.axvline(best_lag, color='red', linestyle=':', alpha=0.6)

    # Título embutido
    ax.set_title(f'{var_x}  ↔  {var_y}   •   rₘₐₓ={best_r:.3f}  @ lag={best_lag} d')

    ax.set_xlabel('Defasagem (dias)')
    ax.set_ylabel('r(lag)')
    ax.grid(alpha=0.3)

plt.tight_layout()
# plt.savefig('figura6_crosscorrelation.png', dpi=300)
plt.show()






fim = time.time()
print(':',fim -inicio)
